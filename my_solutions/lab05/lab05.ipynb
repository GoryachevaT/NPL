{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import json\n",
    "import math\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plot\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.grid_search import GridSearchCV#\n",
    "\n",
    "import os, fnmatch\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = '/data/share/lab05data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = fnmatch.filter(os.listdir(mypath), 'base_*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ids = [514, 3489, 9, 2570, 3084, 3086, 1040, 1113, 3603, 2582, 1559, 2591, 2081, 2602, 1281, 1289, 1076, 569, 2618, 572, 2622, 1599, 2100, 579, 581, 586, 1720, 78, 1616, 3665, 174, 85, 598, 599, 89, 1626, 2139, 3600, 2144, 1121, 1123, 2374, 2066, 3178, 1131, 2067, 624, 3433, 1139, 1658, 1659, 2173, 1151, 1664, 3719, 2696, 2185, 1932, 3214, 656, 1174, 3224, 665, 1691, 160, 161, 3041, 3239, 1704, 683, 1651, 1200, 2225, 2232, 695, 3256, 698, 1211, 1215, 1728, 3268, 2758, 3783, 715, 1230, 1232, 2258, 1806, 219, 1244, 1757, 808, 1262, 2799, 3824, 60, 1779, 1441, 1270, 1697, 257, 3332, 1798, 775, 2313, 1294, 273, 275, 2324, 3349, 3352, 1331, 1831, 3368, 3293, 307, 2864, 2353, 2579, 3892, 2869, 2362, 2876, 1343, 3904, 322, 836, 2885, 3398, 3913, 1355, 3405, 1360, 2386, 3925, 2902, 3415, 857, 347, 144, 2806, 3940, 999, 402, 2921, 1909, 1911, 377, 1406, 1407, 384, 2950, 903, 1928, 3468, 910, 1423, 3472, 2450, 406, 409, 2973, 2462, 1439, 2464, 2977, 2896, 1956, 1961, 1437, 427, 429, 1524, 950, 958, 758, 3233, 3524, 3013, 2504, 3530, 3020, 464, 2517, 3030, 3545, 336, 989, 3550, 479, 1951, 482, 2535, 2538, 1501, 3570, 2036, 887, 1016, 2200]\n",
    "test = []\n",
    "for i in test_ids:\n",
    "    test.append('test_' + str(i) + '.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_texts = []\n",
    "for text in base:\n",
    "    with open(mypath + '/' + text, 'r', encoding='utf-8') as f:\n",
    "        base_texts.append(f.read())\n",
    "\n",
    "raw_test_texts = []\n",
    "for text in test:\n",
    "    with open(mypath + '/' + text, 'r', encoding='utf-8') as f:\n",
    "        raw_test_texts.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/bd9/lib/python3.6/site-packages/bs4/__init__.py:181: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "The code that caused this warning is on line 193 of the file /opt/anaconda/envs/bd9/lib/python3.6/runpy.py. To get rid of this warning, change code that looks like this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP})\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup(YOUR_MARKUP, \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "norm_base_texts = []\n",
    "for text in base_texts:\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    norm_base_texts.append(text.lower())\n",
    "    \n",
    "norm_test_texts = []\n",
    "for text in raw_test_texts:\n",
    "    text = BeautifulSoup(text).get_text()\n",
    "    norm_test_texts.append(text.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Все слова в кучу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymystem3\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "mystem = pymystem3.Mystem()\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import RegexpTokenizer\n",
    "# tokenizer = RegexpTokenizer('[A-Za-zА-Яа-яёЁ]+')\n",
    "\n",
    "# def tokenize(text):\n",
    "#     tokens = tokenizer.tokenize(text)\n",
    "#     for i in tokens:\n",
    "#         if len(i)>2:\n",
    "#             yield m.lemmatize(i)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_wordlist(text):\n",
    "    text = re.sub('n\\'t', ' not', text)\n",
    "    text = re.sub('[^a-zA-Zа-яА-Я]', ' ', text)\n",
    "    words = text.lower().split()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = []\n",
    "for item in norm_base_texts:\n",
    "    new_item = text_to_wordlist(item)\n",
    "    lemm_mystem = [mystem.lemmatize(x)[0] for x in new_item]\n",
    "    tokens_stem = [wordnet_lemmatizer.lemmatize(x, pos=wordnet.VERB) for x in lemm_mystem]\n",
    "    train_texts.append(tokens_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = []\n",
    "for item in norm_test_texts:\n",
    "    new_item = text_to_wordlist(item)\n",
    "    lemm_mystem = [mystem.lemmatize(x)[0] for x in new_item]\n",
    "    tokens_stem = [wordnet_lemmatizer.lemmatize(x, pos=wordnet.VERB) for x in lemm_mystem]\n",
    "    test_texts.append(tokens_stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [\"еще\", \"него\", \"сказать\", \"а\", \"ж\", \"нее\", \"со\", \"без\", \"же\", \"ней\", \"совсем\", \"более\", \"жизнь\", \n",
    "              \"нельзя\", \"так\", \"больше\", \"за\", \"нет\", \"такой\", \"будет\", \"зачем\", \"ни\", \"там\", \"будто\", \"здесь\", \n",
    "              \"нибудь\", \"тебя\", \"бы\", \"и\", \"никогда\", \"тем\", \"был\", \"из\", \"ним\", \"теперь\", \"была\", \"из\", \"за\",\n",
    "              \"них\", \"то\", \"были\", \"или\", \"ничего\", \"тогда\", \"было\", \"им\", \"но\", \"того\", \"быть\", \"иногда\", \"ну\", \n",
    "              \"тоже\", \"в\", \"их\", \"о\", \"только\", \"вам\", \"к\", \"об\", \"том\", \"вас\", \"кажется\", \"один\", \"тот\", \"вдруг\",\n",
    "              \"как\", \"он\", \"три\", \"ведь\", \"какая\", \"она\", \"тут\", \"во\", \"какой\", \"они\", \"ты\", \"вот\", \"когда\", \"опять\",\n",
    "              \"у\", \"впрочем\", \"конечно\", \"от\", \"уж\", \"все\", \"которого\", \"перед\", \"уже\", \"всегда\", \"которые\", \"по\",\n",
    "              \"хорошо\", \"всего\", \"кто\", \"под\", \"хоть\", \"всех\", \"куда\", \"после\", \"чего\", \"всю\", \"ли\", \"потом\", \"человек\",\n",
    "              \"вы\", \"лучше\", \"потому\", \"чем\", \"г\", \"между\", \"почти\", \"через\", \"где\", \"меня\", \"при\", \"что\", \"говорил\",\n",
    "              \"мне\", \"про\", \"чтоб\", \"да\", \"много\", \"раз\", \"чтобы\", \"даже\", \"может\", \"разве\", \"чуть\", \"два\", \"можно\",\n",
    "              \"с\", \"эти\", \"для\", \"мой\", \"сам\", \"этого\", \"до\", \"моя\", \"свое\", \"этой\", \"другой\", \"мы\", \"свою\", \"этом\",\n",
    "              \"его\", \"на\", \"себе\", \"этот\", \"ее\", \"над\", \"себя\", \"эту\", \"ей\", \"надо\", \"сегодня\", \"я\", \"ему\", \"наконец\",\n",
    "              \"сейчас\", \"если\", \"нас\", \"сказал\", \"есть\", \"не\", \"сказала\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentence in train_texts:\n",
    "    for word in sentence:\n",
    "        if len(word) <= 3:\n",
    "            sentence.remove(word)\n",
    "        if word in stop_words and word in sentence:\n",
    "            sentence.remove(word)\n",
    "            \n",
    "for sentence in test_texts:\n",
    "    for word in sentence:\n",
    "        if len(word) <= 3:\n",
    "            sentence.remove(word)\n",
    "        if word in stop_words and word in sentence:\n",
    "            sentence.remove(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20 ms, sys: 4 ms, total: 24 ms\n",
      "Wall time: 26 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models.word2vec import Word2Vec \n",
    "\n",
    "params = {}\n",
    "\n",
    "params['size'] = 300  # итоговая размерность вектора каждого слова\n",
    "params['min_count'] = 5  # минимальная частотность слова, чтобы оно попало в модель\n",
    "params['workers'] = 8     # количество ядер вашего процессора, чтоб запустить обучение в несколько потоков\n",
    "\n",
    "\n",
    "model = Word2Vec(train_texts, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Схожесть текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_to_vec(words, model, size):\n",
    "    text_vec = np.zeros((size,), dtype=\"float32\")\n",
    "    n_words = 0\n",
    "\n",
    "    index2word_set = set(model.wv.vocab.keys())\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            n_words = n_words + 1\n",
    "            text_vec = np.add(text_vec, model[word])\n",
    "    \n",
    "    if n_words != 0:\n",
    "        text_vec /= n_words\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "def texts_to_vecs(texts, model, size):\n",
    "    texts_vecs = np.zeros((len(texts), size), dtype=\"float32\")\n",
    "    \n",
    "    for i, text in tqdm(enumerate(texts),):\n",
    "        texts_vecs[i] = text_to_vec(text, model, size)\n",
    "\n",
    "    return texts_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67676e1e869a4c388f252d333a67a254",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/bd9/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "train_vecs = texts_to_vecs(train_texts, model, params['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac4dd7617f8a405a80509a78a93d607e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda/envs/bd9/lib/python3.6/site-packages/ipykernel_launcher.py:11: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "test_vecs = texts_to_vecs(test_texts, model, params['size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "\n",
    "    def dot_product2(v1, v2):\n",
    "        return sum(map(operator.mul, v1, v2))\n",
    "\n",
    "    def vector_cos5(v1, v2):\n",
    "        prod = dot_product2(v1, v2)\n",
    "        len1 = math.sqrt(dot_product2(v1, v1))\n",
    "        len2 = math.sqrt(dot_product2(v2, v2))\n",
    "        return prod / (len1 * len2)\n",
    "\n",
    "    return vector_cos5(vec1, vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity = []\n",
    "\n",
    "for test in test_vecs:\n",
    "    tmp = 0\n",
    "    for train in train_vecs:\n",
    "        tmp += cosine_similarity(train, test)\n",
    "    similarity.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = sum(similarity)/len(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_sim = [1 if x >= mean else 0 for x in similarity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ones = [i[0] for i in list(zip(test_ids, is_sim)) if i[1] == 1]\n",
    "zeros = [i[0] for i in list(zip(test_ids, is_sim)) if i[1] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin_json =  { \"defined\" : ones, \"other\" : zeros} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../lab05.json', 'w') as outfile:\n",
    "    json.dump(fin_json, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
