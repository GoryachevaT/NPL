{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.3\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.6.5 (default, Apr 29 2018 16:14:56)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import sys\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"]='--conf spark.sql.catalogImplementation=in-memory pyspark-shell'\n",
    "os.environ[\"PYSPARK_PYTHON\"]='/opt/anaconda/envs/bd9/bin/python'\n",
    "os.environ[\"SPARK_HOME\"]='/usr/hdp/current/spark2-client'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "if not spark_home:\n",
    "    raise ValueError('SPARK_HOME environment variable is not set')\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python'))\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.10.7-src.zip'))\n",
    "exec(open(os.path.join(spark_home, 'python/pyspark/shell.py')).read())\n",
    "\n",
    "\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext, SQLContext\n",
    "from pyspark import sql\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Load the whole datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join('/labs/lab10data/', 'lab10_train.csv')\n",
    "item_path = os.path.join('/labs/lab10data/', 'lab10_items.csv')\n",
    "test_path = os.path.join('/labs/lab10data/', 'lab10_test.csv')\n",
    "views_programmes = os.path.join('/labs/lab10data', 'lab10_views_programmes.csv')\n",
    "res_path = os.path.join('/user/artem.moskalets/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fields = [StructField('user_id', LongType(), True),\n",
    "                StructField('item_id', LongType(), True),\n",
    "               StructField('purchase', ByteType(), True)]\n",
    "schema_train = StructType(train_fields)\n",
    "df_train = spark.read.csv(header = 'true', schema = schema_train, path = train_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- item_id: long (nullable = true)\n",
      " |-- purchase: byte (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+\n",
      "|user_id|item_id|purchase|\n",
      "+-------+-------+--------+\n",
      "|   1654|  74107|       0|\n",
      "|   1654|  89249|       0|\n",
      "|   1654|  99982|       0|\n",
      "|   1654|  89901|       0|\n",
      "|   1654| 100504|       0|\n",
      "|   1654|  66187|       0|\n",
      "|   1654|  84350|       0|\n",
      "|   1654|  92854|       0|\n",
      "|   1654|  72811|       0|\n",
      "|   1654|  86876|       0|\n",
      "|   1654| 102657|       0|\n",
      "|   1654| 100482|       0|\n",
      "|   1654|  89677|       0|\n",
      "|   1654|  99419|       0|\n",
      "|   1654|  66603|       0|\n",
      "|   1654|   7363|       0|\n",
      "|   1654|   1320|       0|\n",
      "|   1654|  88892|       0|\n",
      "|   1654|  66671|       0|\n",
      "|   1654|  75925|       0|\n",
      "+-------+-------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_fields = [StructField('item_id', LongType(), True),\n",
    "               StructField('channel_id', DoubleType(), True),\n",
    "               StructField('datetime_availability_start', TimestampType(), True),\n",
    "               StructField('datetime_availability_stop', TimestampType(), True),\n",
    "               StructField('datetime_show_start', TimestampType(), True),\n",
    "               StructField('datetime_show_stop', TimestampType(), True),\n",
    "               StructField('content_type', ByteType(), True),\n",
    "               StructField('title', StringType(), True),\n",
    "               StructField('year', DoubleType(), True),\n",
    "               StructField('genres', StringType(), True),\n",
    "               StructField('region_id', FloatType(), True)]               \n",
    "schema_item = StructType(item_fields)\n",
    "df_item = spark.read.csv(header = 'true', schema = schema_item, path = item_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              genres|\n",
      "+--------------------+\n",
      "|             Эротика|\n",
      "|             Эротика|\n",
      "|             Эротика|\n",
      "|             Эротика|\n",
      "|             Эротика|\n",
      "|             Комедии|\n",
      "|   Комедии,Мелодрамы|\n",
      "|Ужасы,Триллеры,Др...|\n",
      "|Ужасы,Комедии,Фан...|\n",
      "|Комедии,Мелодрамы...|\n",
      "|Детективы,Триллер...|\n",
      "|Фантастика,Боевик...|\n",
      "|Детективы,Триллер...|\n",
      "|Детективы,Триллер...|\n",
      "|Ужасы,Детективы,Т...|\n",
      "|Комедии,Драмы,Зар...|\n",
      "|Военные,Боевики,Наши|\n",
      "|Приключения,Истор...|\n",
      "|Военные,Драмы,Ист...|\n",
      "|Комедии,Драмы,Мел...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = df_item. select('genres')\n",
    "names.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- genres: string (nullable = true)\n",
      " |-- genres_s: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "names = names.withColumn('genres_s',F.split(\"genres\",','))\n",
    "names.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = names.select(F.explode('genres_s').alias('genres_s')).groupby().agg(F.collect_set('genres_s'))\n",
    "names_genres = names.rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " сказка\n",
      "General\n",
      "Анимация\n",
      "Аниме\n",
      "Арт-хаус\n",
      "Артхаус\n",
      "Биография\n",
      "Боевик\n",
      "Боевики\n",
      "Вестерн\n",
      "Видеоигры\n",
      "Военные\n",
      "Военный\n",
      "Детективы\n",
      "Детские\n",
      "Детские песни\n",
      "Для взрослых\n",
      "Для всей семьи\n",
      "Для детей\n",
      "Для самых маленьких\n",
      "Документальные\n",
      "Документальный\n",
      "Драма\n",
      "Драмы\n",
      "Западные мультфильмы\n",
      "Зарубежные\n",
      "Игры\n",
      "Исторические\n",
      "Исторический\n",
      "Комедии\n",
      "Комедия\n",
      "Короткометражки\n",
      "Короткометражные\n",
      "Криминал\n",
      "Кулинария\n",
      "Мелодрама\n",
      "Мелодрамы\n",
      "Мистические\n",
      "Музыкальные\n",
      "Музыкальный\n",
      "Мультсериалы\n",
      "Мультфильм\n",
      "Мультфильмы\n",
      "Мультфильмы в 3D\n",
      "Мюзиклы\n",
      "Научная фантастика\n",
      "Наши\n",
      "О здоровье\n",
      "Охота и рыбалка\n",
      "Передачи\n",
      "Познавательные\n",
      "Полнометражные\n",
      "Приключение\n",
      "Приключения\n",
      "Про животных\n",
      "Прочие\n",
      "Развивающие\n",
      "Развлекательные\n",
      "Реалити-шоу\n",
      "Романтические\n",
      "Русские\n",
      "Русские мультфильмы\n",
      "Семейные\n",
      "Семейный\n",
      "Сериалы\n",
      "Сказки\n",
      "Советские\n",
      "Советское кино\n",
      "Союзмультфильм\n",
      "Спорт\n",
      "Спортивные\n",
      "Триллер\n",
      "Триллеры\n",
      "Ужасы\n",
      "Фантастика\n",
      "Фантастические\n",
      "Фильмы\n",
      "Фильмы в 3D\n",
      "Фильмы-спектакли\n",
      "Фэнтези\n",
      "Хочу всё знать\n",
      "Экранизации\n",
      "Эротика\n",
      "Юмористические\n"
     ]
    }
   ],
   "source": [
    "for i in names_genres:\n",
    "    for j in sorted(i):\n",
    "        print (j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {u'арт-хаус': u'артхаус',             \n",
    "u'военный': u'военные',\\\n",
    "u'боевики': u'боевик',\\\n",
    "u'вестерн': u'вестерн',\\\n",
    "u'видеоигры': u'видеоигры, игры',\\\n",
    "u'детективы':u'детективы',\\\n",
    "u'документальный': u'документальные',\\\n",
    "u'драмы': u'драма',\\\n",
    "u'исторический': u'исторические',\\\n",
    "u'комедия': u'комедии',\\\n",
    "u'короткометражные':u'короткометражки',\n",
    "u'мелодрама': u'мелодрамы',\\\n",
    "u'мистические': u'мистические',\\\n",
    "u'музыкальные': u'музыкальный',\\\n",
    "u'мьюзиклы': u'мьюзиклы,музыкальный',\\\n",
    "u'приключение': u'приключения',\\\n",
    "u'романтические': u'мелодрамы',\\\n",
    "u'семейный': u'семейные',\\\n",
    "u'для всей семьи': u'семейные',\\\n",
    "u'советское кино': u'советские, русские',\\\n",
    "u'наши': u'русские',\\\n",
    "u'спортивные': u'спорт',\\\n",
    "u'ужасы': u'триллер, ужасы',\\\n",
    "u'фантастика': u'фантастические',\\\n",
    "u'фильмы в 3d': u'фильмы,3d',\\\n",
    "u'фэнтези': u'фэнтези,фантастические',\\\n",
    "u'научная фантастика': u'научные,фантастические',\\\n",
    "u'хочу всё знать': u'познавательные, развивающие',\\\n",
    "u'познавательные': u'познавательные, развивающие',\\\n",
    "u'детские песни': u'детские,песни',\\\n",
    "u'для детей': u'детские',\\\n",
    "u'для самых маленьких': u'детские',\\\n",
    "u'западные мультфильмы': u'детские, мультфильм,западные',\\\n",
    "u'западные детские' : u'западные, детские',\n",
    "u'русские детские' : u'русские, детские',             \n",
    "u'зарубежные': u'зарубежные,западные',\n",
    "u'мультсериалы': u'детские, мультфильм',\\\n",
    "u'мультфильмы': u'детские,мультфильм',\\\n",
    "u'мультфильм в 3d': u'детские,мультфильм, 3d',\\\n",
    "u'русские мультфильмы': u'детские,мультфильм,русские',\\\n",
    "u'развивающие': u'детские,развивающие',\\\n",
    "u'сказки': u'детские,сказки',\\\n",
    "u'сказка': u'детские,сказки',\\\n",
    "u'триллеры': u'триллер',\\\n",
    "u'союзмультфильм': u'детские,мультфильм',\\\n",
    "u'анимация': u'детские,мультфильм',\\\n",
    "u'эротика': u'эротика, для взрослых'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fields = [StructField('user_id', LongType(), True),\n",
    "              StructField('item_id', LongType(), True),\n",
    "              StructField('purchase', ByteType(), True)]\n",
    "schema_test = StructType(test_fields)\n",
    "df_test = spark.read.csv(header = 'true', schema = schema_test, path = test_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- item_id: long (nullable = true)\n",
      " |-- purchase: byte (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "views_programmes_fields = [StructField('user_id', LongType(), True),\n",
    "                           StructField('item_id', LongType(), True),\n",
    "                           StructField('ts_start', LongType(), True),\n",
    "                           StructField('ts_end', LongType(), True),\n",
    "                           StructField('item_type', StringType(), True)]\n",
    "schema_views_programmes = StructType(views_programmes_fields)\n",
    "df_views_programmes = spark.read.csv(header = 'true', schema = schema_views_programmes, path = views_programmes, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- item_id: long (nullable = true)\n",
      " |-- ts_start: long (nullable = true)\n",
      " |-- ts_end: long (nullable = true)\n",
      " |-- item_type: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_views_programmes.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_train.join(df_item, df_train.item_id == df_item.item_id, \"inner\").drop(df_item.item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features.select('user_id','item_id','purchase','year','genres')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features.select('*', coalesce(df_features[\"year\"], lit(0.0)).alias('year_0'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features.drop(\"year\").select('*', col('year_0').alias(\"year\")).drop('year_0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features.fillna('noname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen(films):\n",
    "    lst =  films.split(',')\n",
    "    res = ''\n",
    "    for l in lst:\n",
    "        l = l.lower().strip()\n",
    "        for key in list(dictionary.keys()):\n",
    "            l = l.replace(key, dictionary[key])\n",
    "        res = res + ','+ l +','\n",
    "    return res\n",
    "gen =  udf(gen,StringType())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features.withColumn('Words', gen(df_features.genres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               Words|\n",
      "+--------------------+\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "|,триллер, ужасы,,...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.select(df_features.Words).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def un(one, two, three, four):\n",
    "    return str(one)+','+str(two) +','+str(three)+','+(four)\n",
    "un = udf(un,StringType())\n",
    "df_features = df_features.withColumn('all_features', un('user_id','item_id','year','words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------+\n",
      "|all_features                                                            |\n",
      "+------------------------------------------------------------------------+\n",
      "|1654,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,  |\n",
      "|510087,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|522798,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|529632,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|566758,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|613775,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|619378,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|625638,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|632495,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|639612,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|651811,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|659698,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|706816,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|717302,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|719149,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|725821,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|728909,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|728960,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|729785,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "|731490,3764,2013.0,,триллер, ужасы,,триллер,,драма,,зарубежные,западные,|\n",
      "+------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.select('all_features').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5032624"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features.select(df_features.Words).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, RegexTokenizer\n",
    "\n",
    "tokenizer = RegexTokenizer(inputCol=\"all_features\", \n",
    "                           outputCol=\"lst\", \n",
    "                           gaps=False, \n",
    "                           pattern=r'[a-zA-Zа-яА-Яё0-9_\\-]{2,}')\n",
    "df_features = tokenizer.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+\n",
      "|lst                                                                       |\n",
      "+--------------------------------------------------------------------------+\n",
      "|[836538, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[836889, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[836923, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[836953, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[837038, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[837166, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[837219, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[837578, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[837780, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[837850, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[837992, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[838210, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[838316, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[838373, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[838617, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[838645, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[839229, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[839234, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[839593, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "|[840014, 3764, 2013, триллер, ужасы, триллер, драма, зарубежные, западные]|\n",
      "+--------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.select('lst').show(truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.linalg import Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol= \"lst\", outputCol=\"features\")\n",
    "model_cv = cv.fit(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = model_cv.transform(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------+--------------------+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|user_id|item_id|purchase|              genres|  year|               Words|        all_features|                 lst|            features|\n",
      "+-------+-------+--------+--------------------+------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 909525|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|909525,3764,2013....|[909525, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 909582|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|909582,3764,2013....|[909582, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 909603|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|909603,3764,2013....|[909603, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 909741|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|909741,3764,2013....|[909741, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 909745|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|909745,3764,2013....|[909745, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 909786|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|909786,3764,2013....|[909786, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 909901|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|909901,3764,2013....|[909901, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 909961|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|909961,3764,2013....|[909961, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 910008|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|910008,3764,2013....|[910008, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 910026|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|910026,3764,2013....|[910026, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 910039|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|910039,3764,2013....|[910039, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 910185|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|910185,3764,2013....|[910185, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 910209|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|910209,3764,2013....|[910209, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 910345|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|910345,3764,2013....|[910345, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 910361|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|910361,3764,2013....|[910361, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 910469|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|910469,3764,2013....|[910469, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 910496|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|910496,3764,2013....|[910496, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 910539|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|910539,3764,2013....|[910539, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 910553|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|910553,3764,2013....|[910553, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "| 910702|   3764|       0|Ужасы,Триллеры,Др...|2013.0|,триллер, ужасы,,...|910702,3764,2013....|[910702, 3764, 20...|(5783,[0,1,4,5,12...|\n",
      "+-------+-------+--------+--------------------+------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = df_features.drop('genres','Words','user_id','item_id','all_features','genres','year','lst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|purchase|            features|\n",
      "+--------+--------------------+\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "|       0|(5783,[0,1,4,5,12...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"purchase\", outputCol=\"indexedLabel\").fit(df_features)\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\").fit(df_features)\n",
    "\n",
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = df_features.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier, RandomForestClassificationModel\n",
    "\n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(numTrees = 800, maxDepth = 12, labelCol=\"purchase\", featuresCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = rf.fit(df_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('RF_lab10_3.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassificationModel.load('RF_lab10_3.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+\n",
      "|user_id|item_id|            features|\n",
      "+-------+-------+--------------------+\n",
      "| 517612|   3764|(5783,[0,1,4,5,12...|\n",
      "| 520446|   3764|(5783,[0,1,4,5,12...|\n",
      "| 523860|   3764|(5783,[0,1,4,5,12...|\n",
      "| 556825|   3764|(5783,[0,1,4,5,12...|\n",
      "| 566701|   3764|(5783,[0,1,4,5,12...|\n",
      "| 575248|   3764|(5783,[0,1,4,5,12...|\n",
      "| 588378|   3764|(5783,[0,1,4,5,12...|\n",
      "| 625678|   3764|(5783,[0,1,4,5,12...|\n",
      "| 627053|   3764|(5783,[0,1,4,5,12...|\n",
      "| 636572|   3764|(5783,[0,1,4,5,12...|\n",
      "| 642397|   3764|(5783,[0,1,4,5,12...|\n",
      "| 668112|   3764|(5783,[0,1,4,5,12...|\n",
      "| 703514|   3764|(5783,[0,1,4,5,12...|\n",
      "| 711308|   3764|(5783,[0,1,4,5,12...|\n",
      "| 736010|   3764|(5783,[0,1,4,5,12...|\n",
      "| 739215|   3764|(5783,[0,1,4,5,12...|\n",
      "| 740405|   3764|(5783,[0,1,4,5,12...|\n",
      "| 742324|   3764|(5783,[0,1,4,5,12...|\n",
      "| 742986|   3764|(5783,[0,1,4,5,12...|\n",
      "| 746959|   3764|(5783,[0,1,4,5,12...|\n",
      "+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.csv(header = 'true', schema = schema_test, path = test_path, sep=',')\n",
    "df_test = df_test.join(df_item, df_test.item_id == df_item.item_id, \"inner\").drop(df_item.item_id)\n",
    "df_test = df_test.select('user_id','item_id','year','genres')\n",
    "df_test = df_test.select('*', coalesce(df_test[\"year\"], lit(0.0)).alias('year_0'))\n",
    "df_test = df_test.drop(\"year\").select('*', col('year_0').alias(\"year\")).drop('year_0')\n",
    "df_test = df_test.fillna('noname')\n",
    "df_test = df_test.withColumn('Words', gen(df_test.genres))\n",
    "df_test = df_test.withColumn('all_features', un('user_id','item_id','year','words'))\n",
    "df_test = tokenizer.transform(df_test)\n",
    "df_test = model_cv.transform(df_test)\n",
    "df_test = df_test.drop('genres','Words','all_features','genres','year','lst','purchase')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+\n",
      "|user_id|item_id|            features|\n",
      "+-------+-------+--------------------+\n",
      "| 874011|   3764|(5783,[0,1,4,5,12...|\n",
      "| 874417|   3764|(5783,[0,1,4,5,12...|\n",
      "| 874709|   3764|(5783,[0,1,4,5,12...|\n",
      "| 875202|   3764|(5783,[0,1,4,5,12...|\n",
      "| 875222|   3764|(5783,[0,1,4,5,12...|\n",
      "| 875274|   3764|(5783,[0,1,4,5,12...|\n",
      "| 875552|   3764|(5783,[0,1,4,5,12...|\n",
      "| 875580|   3764|(5783,[0,1,4,5,12...|\n",
      "| 875609|   3764|(5783,[0,1,4,5,12...|\n",
      "| 875996|   3764|(5783,[0,1,4,5,12...|\n",
      "| 876215|   3764|(5783,[0,1,4,5,12...|\n",
      "| 876221|   3764|(5783,[0,1,4,5,12...|\n",
      "| 876234|   3764|(5783,[0,1,4,5,12...|\n",
      "| 876263|   3764|(5783,[0,1,4,5,12...|\n",
      "| 876350|   3764|(5783,[0,1,4,5,12...|\n",
      "| 876426|   3764|(5783,[0,1,4,5,12...|\n",
      "| 876455|   3764|(5783,[0,1,4,5,12...|\n",
      "| 876758|   3764|(5783,[0,1,4,5,12...|\n",
      "| 876896|   3764|(5783,[0,1,4,5,12...|\n",
      "| 877001|   3764|(5783,[0,1,4,5,12...|\n",
      "+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2156840"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+\n",
      "|user_id|item_id|            features|\n",
      "+-------+-------+--------------------+\n",
      "| 517612|   3764|(5783,[0,1,4,5,12...|\n",
      "| 520446|   3764|(5783,[0,1,4,5,12...|\n",
      "| 523860|   3764|(5783,[0,1,4,5,12...|\n",
      "| 556825|   3764|(5783,[0,1,4,5,12...|\n",
      "| 566701|   3764|(5783,[0,1,4,5,12...|\n",
      "| 575248|   3764|(5783,[0,1,4,5,12...|\n",
      "| 588378|   3764|(5783,[0,1,4,5,12...|\n",
      "| 625678|   3764|(5783,[0,1,4,5,12...|\n",
      "| 627053|   3764|(5783,[0,1,4,5,12...|\n",
      "| 636572|   3764|(5783,[0,1,4,5,12...|\n",
      "| 642397|   3764|(5783,[0,1,4,5,12...|\n",
      "| 668112|   3764|(5783,[0,1,4,5,12...|\n",
      "| 703514|   3764|(5783,[0,1,4,5,12...|\n",
      "| 711308|   3764|(5783,[0,1,4,5,12...|\n",
      "| 736010|   3764|(5783,[0,1,4,5,12...|\n",
      "| 739215|   3764|(5783,[0,1,4,5,12...|\n",
      "| 740405|   3764|(5783,[0,1,4,5,12...|\n",
      "| 742324|   3764|(5783,[0,1,4,5,12...|\n",
      "| 742986|   3764|(5783,[0,1,4,5,12...|\n",
      "| 746959|   3764|(5783,[0,1,4,5,12...|\n",
      "+-------+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_test.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proba1(vector):\n",
    "    from pyspark.ml.linalg import Vectors\n",
    "    res = float(vector[1])\n",
    "    return res\n",
    "proba1 = udf(proba1, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions1 = predictions.select('*', proba1('probability').alias('purchase'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+--------------------+--------------------+--------------------+----------+------------+\n",
      "|user_id|item_id|            features|       rawPrediction|         probability|prediction|    purchase|\n",
      "+-------+-------+--------------------+--------------------+--------------------+----------+------------+\n",
      "| 874011|   3764|(5783,[0,1,4,5,12...|[798.359971866320...|[0.99794996483290...|       0.0|0.0020500352|\n",
      "| 874417|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 874709|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 875202|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 875222|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 875274|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 875552|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 875580|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 875609|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 875996|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 876215|   3764|(5783,[0,1,4,5,12...|[798.294496742290...|[0.99786812092786...|       0.0| 0.002131879|\n",
      "| 876221|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 876234|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 876263|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 876350|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 876426|   3764|(5783,[0,1,4,5,12...|[798.326160508788...|[0.99790770063598...|       0.0|0.0020922993|\n",
      "| 876455|   3764|(5783,[0,1,4,5,12...|[798.363296541461...|[0.99795412067682...|       0.0|0.0020458794|\n",
      "| 876758|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 876896|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "| 877001|   3764|(5783,[0,1,4,5,12...|[798.364479377077...|[0.99795559922134...|       0.0|0.0020444007|\n",
      "+-------+-------+--------------------+--------------------+--------------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2156840"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "Res = predictions1.select(\"user_id\",\"item_id\",\"purchase\").\\\n",
    "                    sort(\"user_id\",\"item_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "Res.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"lab10_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hadoop fs -getmerge lab10_3.csv lab10s.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
